{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c4d952877ba27b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:57:48.690805Z",
     "start_time": "2024-01-11T19:57:48.636693100Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:57:51.920792900Z",
     "start_time": "2024-01-11T19:57:49.358322Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.cuda\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e087243cce49ac99",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:57:54.733405300Z",
     "start_time": "2024-01-11T19:57:54.598443100Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"0\":\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8fce2a875401a7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T19:57:56.112544400Z",
     "start_time": "2024-01-11T19:57:55.997476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'0'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b31d0703869d25c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T08:42:29.055051500Z",
     "start_time": "2024-01-11T08:42:28.912061800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.pt\")\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d233d92d3400a5b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:32:05.101788400Z",
     "start_time": "2024-01-10T13:26:10.378547100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.0 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.235 ðŸš€ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "WARNING âš ï¸ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0mtask=detect, mode=train, model=yolov8n.pt, data=dataset/data.yaml, epochs=15, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3012408 parameters, 3012392 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning D:\\Programming\\Pycharm Projects\\Torch\\Object Detection\\UI-Recon\\dataset\\train\\labels... 1452 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1452/1452 [00:00<00:00, 2326.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mNew cache created: D:\\Programming\\Pycharm Projects\\Torch\\Object Detection\\UI-Recon\\dataset\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mScanning D:\\Programming\\Pycharm Projects\\Torch\\Object Detection\\UI-Recon\\dataset\\valid\\labels... 235 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [00:00<00:00, 1052.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mNew cache created: D:\\Programming\\Pycharm Projects\\Torch\\Object Detection\\UI-Recon\\dataset\\valid\\labels.cache\n",
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "15 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/15      2.83G      1.356      2.622      1.214        319        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:18<00:00,  4.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663       0.66       0.43      0.454       0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/15      2.99G      1.026      1.362      1.059        240        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:17<00:00,  5.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.695      0.574      0.599      0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/15      2.95G     0.9466      1.158      1.012        297        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:16<00:00,  5.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.625      0.571      0.595      0.435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/15      2.46G     0.9041      1.058     0.9906        220        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:16<00:00,  5.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.602       0.69      0.669      0.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/15       2.6G      0.853     0.9884      0.973        275        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:16<00:00,  5.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.643      0.714      0.713      0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/15      2.17G     0.8777      1.041     0.9877        139        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:14<00:00,  6.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.689      0.664      0.711      0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/15      2.17G     0.8431     0.9529     0.9629        146        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:12<00:00,  7.54it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.627      0.748      0.696      0.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/15      2.21G     0.7994     0.8789     0.9469        169        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:12<00:00,  7.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.739      0.751      0.781      0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/15      2.29G     0.7713     0.8395     0.9336        144        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:12<00:00,  7.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.782      0.742      0.809      0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/15      2.18G     0.7395     0.7987     0.9267        152        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:12<00:00,  7.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.763       0.77      0.805       0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/15      2.17G     0.7274     0.7677     0.9204        242        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:12<00:00,  7.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.791      0.793      0.838      0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/15      2.34G     0.7096      0.742     0.9192        152        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:14<00:00,  6.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.828      0.827      0.858      0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/15      2.24G     0.6935     0.7144      0.908        116        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:15<00:00,  5.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.825      0.806      0.856      0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/15      2.28G     0.6662     0.6794     0.9008        146        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:15<00:00,  6.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.835      0.795      0.857      0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/15      2.29G     0.6511     0.6638     0.8944        228        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:15<00:00,  5.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.833      0.819      0.866      0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15 epochs completed in 0.077 hours.\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.235 ðŸš€ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.827      0.823      0.866      0.718\n",
      "       BackgroundImage        235         22      0.826      0.909      0.907      0.804\n",
      "                Drawer        235         86      0.738       0.72      0.825      0.552\n",
      "              EditText        235        212      0.824      0.849      0.909      0.743\n",
      "                  Icon        235        786      0.939      0.906      0.962      0.761\n",
      "                 Image        235        497      0.913      0.899      0.959      0.907\n",
      "                 Modal        235         96       0.46      0.438      0.406      0.237\n",
      "                  Text        235       1646      0.942      0.908      0.968      0.802\n",
      "            TextButton        235        318      0.978      0.957      0.991      0.937\n",
      "Speed: 1.0ms preprocess, 2.0ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001B[1mruns\\detect\\train\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data=\"dataset/data.yaml\", epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2c36859b426e58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T14:15:47.610502400Z",
     "start_time": "2024-01-10T14:15:17.400488600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.235 ðŸš€ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mScanning D:\\Programming\\Pycharm Projects\\Torch\\Object Detection\\UI-Recon\\dataset\\valid\\labels.cache... 235 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        235       3663      0.832      0.822      0.866      0.721\n",
      "       BackgroundImage        235         22      0.829      0.909       0.91       0.81\n",
      "                Drawer        235         86      0.737      0.716      0.825      0.553\n",
      "              EditText        235        212       0.83      0.849      0.909      0.742\n",
      "                  Icon        235        786      0.941      0.905      0.962       0.77\n",
      "                 Image        235        497      0.917      0.899      0.959      0.906\n",
      "                 Modal        235         96      0.476      0.435      0.401      0.236\n",
      "                  Text        235       1646      0.945      0.903      0.968      0.807\n",
      "            TextButton        235        318      0.977      0.956      0.991      0.942\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.0ms loss, 3.1ms postprocess per image\n",
      "Results saved to \u001B[1mruns\\detect\\val\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train/weights/best.pt\")\n",
    "val_results = model.val(data=\"dataset/data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0a6538634d0acf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T14:26:05.788222600Z",
     "start_time": "2024-01-10T14:25:58.824989800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.235 Ã°Å¸Å¡â‚¬ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 168 layers, 3007208 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 D:\\Programming\\Pycharm Projects\\Torch\\Object Detection\\UI-Recon\\test\\test.jpg: 320x640 3 Icons, 21 Images, 23 Texts, 22.3ms\n",
      "Speed: 3.0ms preprocess, 22.3ms inference, 8.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Results saved to \u001B[1mruns\\detect\\predict\u001B[0m\n",
      "Ã°Å¸â€™Â¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=predict model=\"runs/detect/train/weights/best.pt\" data=\"dataset/data.yaml\" source=\"test/test.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca97b84852a305f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T09:21:20.086726300Z",
     "start_time": "2024-01-11T09:21:18.601925200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\Programming\\Pycharm Projects\\Torch\\Hackathons\\DarkPatterns\\test\\test.jpg: 320x640 3 Icons, 21 Images, 23 Texts, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 12.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "[tensor([[1347.2997,   41.4881, 1416.2369,   96.6370]], device='cuda:0'), tensor([[845.7868,  39.3317, 900.0107,  95.1558]], device='cuda:0'), tensor([[664.2099,  51.9183, 735.8386,  96.8570]], device='cuda:0'), tensor([[1011.5595,   41.2662, 1084.2572,   87.4371]], device='cuda:0'), tensor([[ 39.5911, 638.5356, 264.5266, 804.0417]], device='cuda:0'), tensor([[585.2716, 605.4911, 765.1380, 800.7845]], device='cuda:0'), tensor([[1552.7063,   44.4126, 1635.6694,   97.3737]], device='cuda:0'), tensor([[ 13.7967, 200.4805, 822.2614, 485.6375]], device='cuda:0'), tensor([[511.5127,  38.2941, 589.0388, 108.8276]], device='cuda:0'), tensor([[585.1302, 850.7538, 766.4218, 886.2565]], device='cuda:0'), tensor([[325.9825, 847.9601, 505.6477, 891.9698]], device='cuda:0'), tensor([[ 860.4188,  357.3307, 1340.0594,  379.0420]], device='cuda:0'), tensor([[887.2032, 848.7320, 990.7570, 877.0388]], device='cuda:0'), tensor([[295.8689, 598.9433, 559.0204, 817.5566]], device='cuda:0'), tensor([[ 865.4691,  417.7983, 1087.3794,  449.0544]], device='cuda:0'), tensor([[1095.2383,  634.6364, 1317.9261,  801.0081]], device='cuda:0'), tensor([[1393.6603,  847.4114, 1523.9746,  890.3274]], device='cuda:0'), tensor([[1151.6465,  850.1578, 1241.6497,  873.2972]], device='cuda:0'), tensor([[507.4007, 120.8353, 593.9777, 139.0540]], device='cuda:0'), tensor([[1543.5087,  495.7127, 1820.1729,  753.3984]], device='cuda:0'), tensor([[1527.0363,  120.8996, 1663.2253,  138.9568]], device='cuda:0'), tensor([[ 811.5778,  597.4072, 1050.9381,  791.5972]], device='cuda:0'), tensor([[1285.0791,  120.3427, 1479.8330,  140.9467]], device='cuda:0'), tensor([[369.9389, 121.2244, 431.4160, 139.9176]], device='cuda:0'), tensor([[219.3974,  35.5320, 289.0458,  95.2718]], device='cuda:0'), tensor([[375.0315,  37.1783, 423.5585,  95.5736]], device='cuda:0'), tensor([[100.1419, 874.6633, 203.2132, 892.5086]], device='cuda:0'), tensor([[1163.5199,   72.2595, 1231.4147,   96.0725]], device='cuda:0'), tensor([[790.9943, 121.1796, 957.7642, 139.1578]], device='cuda:0'), tensor([[875.8068, 198.7622, 995.6156, 228.8226]], device='cuda:0'), tensor([[583.8240, 852.3304, 762.8126, 875.4808]], device='cuda:0'), tensor([[ 857.9948,  248.5398, 1211.4276,  314.6970]], device='cuda:0'), tensor([[1146.5454,  870.7779, 1242.6648,  892.3809]], device='cuda:0'), tensor([[216.5783, 122.0102, 288.8978, 141.0550]], device='cuda:0'), tensor([[ 36.5560, 540.5811, 257.2890, 562.3521]], device='cuda:0'), tensor([[1113.4873,  419.5708, 1358.8557,  445.9225]], device='cuda:0'), tensor([[1350.7614,  603.4496, 1515.5770,  831.2962]], device='cuda:0'), tensor([[638.0571, 121.0478, 759.4844, 139.8779]], device='cuda:0'), tensor([[888.4961, 858.5798, 990.5931, 891.8502]], device='cuda:0'), tensor([[115.7157, 850.2052, 194.2856, 874.8455]], device='cuda:0'), tensor([[327.1750, 849.0517, 503.2943, 878.4147]], device='cuda:0'), tensor([[918.4711, 196.7826, 998.7832, 229.8044]], device='cuda:0'), tensor([[121.1195, 850.3434, 189.5917, 869.9485]], device='cuda:0'), tensor([[1000.3506,  121.7335, 1096.2672,  140.3517]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "from utils.helper_functions import get_text_bboxes\n",
    "\n",
    "model = YOLO(\"runs/detect/train/weights/best.pt\")\n",
    "source = \"test/test.jpg\"\n",
    "\n",
    "bboxes = get_text_bboxes(model, source)\n",
    "print(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e947531ca4be883",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-11T19:59:02.798817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 D:\\Programming\\Pycharm Projects\\Torch\\Hackathons\\DarkPatterns\\test\\test.jpg: 320x640 3 Icons, 21 Images, 23 Texts, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    }
   ],
   "source": [
    "from utils.helper_functions import get_pred_img\n",
    "\n",
    "model = YOLO(\"runs/detect/train/weights/best.pt\")\n",
    "source = \"test/test.jpg\"\n",
    "save_dir = \"predictions\"\n",
    "\n",
    "get_pred_img(model, source, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9a8f3e6e9c9ce5a9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
